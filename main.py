import argparse
import os
import sys
from pathlib import Path
import torch
from huggingface_hub import snapshot_download
from infrastructure.llm_service import QwenService
from infrastructure.storage_service import StorageService
from application.chat_app import ChatApp

EMBEDDING_REPO_ID = "sentence-transformers/all-MiniLM-L6-v2"

def _find_cached_embedding_snapshot() -> Path | None:
    home = Path(os.environ.get("USERPROFILE", ""))
    local_app_data = Path(os.environ.get("LOCALAPPDATA", ""))
    roots = [
        home / ".cache" / "huggingface" / "hub" / "models--sentence-transformers--all-MiniLM-L6-v2" / "snapshots",
        local_app_data / "huggingface" / "hub" / "models--sentence-transformers--all-MiniLM-L6-v2" / "snapshots",
    ]
    candidates = []
    for root in roots:
        if root.exists():
            candidates.extend([p for p in root.iterdir() if p.is_dir()])
    if not candidates:
        return None
    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return candidates[0]

def _resolve_embedding_model_path(requested_path: str) -> str:
    local_path = Path(requested_path).resolve()
    if local_path.exists():
        return str(local_path)

    cached = _find_cached_embedding_snapshot()
    if cached is not None:
        print(f"ğŸ” [System] ä½¿ç”¨æœ¬åœ° HF ç¼“å­˜ Embeddings æ¨¡å‹: {cached}")
        return str(cached)

    print("â¬‡ï¸ [System] æœªæ‰¾åˆ°æœ¬åœ° Embeddings æ¨¡å‹ï¼Œæ­£åœ¨è‡ªåŠ¨ä¸‹è½½...")
    local_path.parent.mkdir(parents=True, exist_ok=True)
    downloaded = snapshot_download(
        repo_id=EMBEDDING_REPO_ID,
        local_dir=str(local_path),
    )
    print(f"âœ… [System] Embeddings æ¨¡å‹å·²ä¸‹è½½åˆ°: {downloaded}")
    return str(Path(downloaded).resolve())

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--task", type=str)
    parser.add_argument("-y", "--yes", action="store_true")
    parser.add_argument("--model-path", type=str, default="./qwen2.5-3b")
    parser.add_argument("--embedding-model-path", type=str, default="./models/all-MiniLM-L6-v2")
    parser.add_argument("--index-dir", type=str, default=".cache/rag")
    parser.add_argument("--reindex", action="store_true", help="å¼ºåˆ¶é‡å»º RAG ç´¢å¼•")
    parser.add_argument(
        "--offline",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="ç¦»çº¿æ¨¡å¼ï¼ˆé»˜è®¤å¼€å¯ï¼‰ï¼šä»…ä»æœ¬åœ°ç›®å½•åŠ è½½æ¨¡å‹ï¼Œä¸è®¿é—® HF Hub",
    )
    args = parser.parse_args()

    local_model_path = str(Path(args.model_path).resolve())
    if not Path(local_model_path).exists():
        raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {local_model_path}")
    embedding_model_path = _resolve_embedding_model_path(args.embedding_model_path)

    if args.offline:
        os.environ["HF_HUB_OFFLINE"] = "1"
        os.environ["TRANSFORMERS_OFFLINE"] = "1"

    use_4bit = torch.cuda.is_available() and (torch.cuda.get_device_properties(0).total_memory / 1024**3 < 8)

    # åŸºç¡€ç»„ä»¶å®ä¾‹åŒ–
    storage = StorageService()
    
    # 2. é«˜çº§ RAG åˆå§‹åŒ– (ä½¿ç”¨ FAISS å·¥ä¸šçº§å®ç°)
    from infrastructure.vector_store import FaissVectorStore
    from application.rag_engine import RagEngine
    vector_store = FaissVectorStore(
        model_name=embedding_model_path,
        local_files_only=True,
    )
    rag_engine = RagEngine(vector_store)
    index_dir = str(Path(args.index_dir).resolve())
    loaded = (not args.reindex) and vector_store.load(index_dir)
    if loaded:
        print(f"ğŸ”­ [System] å·²åŠ è½½æœ¬åœ° RAG ç´¢å¼•: {index_dir}")
    else:
        print("ğŸ”­ [System] æ­£åœ¨æ„å»ºé¡¹ç›®è¯­ä¹‰ç´¢å¼• (RAG)...")
        rag_engine.index_project(".")
        vector_store.save(index_dir)
        print(f"âœ… [System] RAG ç´¢å¼•å·²ä¿å­˜åˆ°: {index_dir}")
    
    llm = QwenService(local_model_path, use_4bit=use_4bit)
    
    # 3. æ³¨å…¥ RAG åˆ° ChatApp
    app = ChatApp(
        llm,
        storage,
        rag_engine=rag_engine,
        auto_approve=args.yes,
        collect_feedback=(args.task is None),
    )

    # æ³¨å†Œæ‰©å±•åçš„å…¨èƒ½å·¥å…·é›†
    from infrastructure.tools import WebSearchTool, DateTimeTool, ListCurrentDirTool, FileAnalysisTool, PlanTool, GitStatusTool
    app.register_tool(WebSearchTool()) # å¢åŠ è”ç½‘æœç´¢
    app.register_tool(DateTimeTool())
    app.register_tool(ListCurrentDirTool())
    app.register_tool(FileAnalysisTool())
    app.register_tool(GitStatusTool())
    app.register_tool(PlanTool(storage))

    if args.task:
        app.run(args.task)
        sys.exit(0)

    print("\nğŸ’¡ è¾“å…¥ 'exit' é€€å‡ºï¼Œè¾“å…¥ 'clear' æ¸…ç©ºå†å²\n")
    while True:
        user_input = input("ğŸ‘¤ You: ").strip()
        if user_input.lower() in ['exit', 'quit']: break
        if user_input.lower() == 'clear':
            app.clear_history()
            continue
        if user_input: app.run(user_input)

if __name__ == "__main__":
    main()
