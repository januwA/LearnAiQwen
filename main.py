from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextIteratorStreamer
import torch
from typing import List, Dict, Any, Generator
from abc import ABC, abstractmethod
from threading import Thread
import os

# --- Interfaces ---

class ILLMService(ABC):
    @abstractmethod
    def generate_response(self, messages: List[Dict[str, str]], max_new_tokens: int = 512) -> str:
        pass
    
    @abstractmethod
    def generate_stream(self, messages: List[Dict[str, str]], max_new_tokens: int = 512) -> Generator[str, None, None]:
        """æµå¼ç”Ÿæˆæ¥å£"""
        pass

# --- Implementations ---

class QwenService(ILLMService):
    def __init__(self, model_path: str, use_4bit: bool = True):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        print(f"   4-bit é‡åŒ–: {'å¯ç”¨' if use_4bit else 'ç¦ç”¨'}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            padding_side="left"
        )
        
        # å¦‚æœæ²¡ pad_tokenï¼Œç”¨ eos_token ä»£æ›¿
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # é‡åŒ–é…ç½®ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
        if use_4bit:
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=quant_config,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True,  # å‡å°‘ CPU å†…å­˜å ç”¨
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
            )
        
        self.model.eval()
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {next(self.model.parameters()).device}")

    def generate_response(self, messages: List[Dict[str, str]], max_new_tokens: int = 512) -> str:
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer(
            [text], 
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)

        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœæ˜¾å­˜
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        
        # å»é™¤è¾“å…¥éƒ¨åˆ†
        input_length = model_inputs.input_ids.shape[1]
        generated_ids = generated_ids[:, input_length:]
        
        return self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    def generate_stream(self, messages: List[Dict[str, str]], max_new_tokens: int = 512) -> Generator[str, None, None]:
        """æµå¼ç”Ÿæˆ"""
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer(
            [text], 
            return_tensors="pt"
        ).to(self.model.device)
        
        streamer = TextIteratorStreamer(
            self.tokenizer, 
            skip_prompt=True, 
            skip_special_tokens=True
        )
        
        generation_kwargs = dict(
            input_ids=model_inputs.input_ids,
            streamer=streamer,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=self.tokenizer.pad_token_id,
        )
        
        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()
        
        for text in streamer:
            yield text
        
        thread.join()

# --- Application Layer ---

class ChatApp:
    def __init__(self, llm_service: ILLMService):
        self.llm_service = llm_service

    def run(self, prompt: str, stream: bool = True):
        messages = [
            {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
        print(f"\nğŸ‘¤ User: {prompt}")
        print("ğŸ¤– Assistant: ", end="", flush=True)
        
        if stream and hasattr(self.llm_service, 'generate_stream'):
            for chunk in self.llm_service.generate_stream(messages):
                print(chunk, end="", flush=True)
        else:
            response = self.llm_service.generate_response(messages)
            print(response, end="")
        
        print("\n")

# --- Main ---

def main():
    # æ¨¡å‹è·¯å¾„
    local_model_path = r"d:\ajanuw\ai_qwen\qwen2.5-1.5b"
    
    # æ£€æŸ¥æ˜¾å­˜
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ æ˜¾å­˜: {gpu_memory:.1f} GB")
        use_4bit = gpu_memory < 8  # å°äº8GBè‡ªåŠ¨å¯ç”¨4-bit
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPUï¼ˆææ…¢ï¼‰")
        use_4bit = False
    
    try:
        # ä¾èµ–æ³¨å…¥
        llm_service = QwenService(local_model_path, use_4bit=use_4bit)
        app = ChatApp(llm_service)
        
        # äº¤äº’å¼å¯¹è¯
        print("\nğŸ’¡ è¾“å…¥ 'exit' é€€å‡º\n")
        while True:
            user_input = input("ğŸ‘¤ You: ").strip()
            if user_input.lower() in ['exit', 'quit']:
                break
            if user_input:
                app.run(user_input, stream=True)
                
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. å®‰è£…ä¾èµ–: uv add transformers torch accelerate bitsandbytes")
        print("3. é™ä½ max_new_tokens æˆ–å¯ç”¨ 4-bit é‡åŒ–")

if __name__ == "__main__":
    main()
